import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# ğŸ’¡ íŒŒìƒ ì§€í‘œ ìƒì„± í•¨ìˆ˜
def add_features(df):
    df['range_pct'] = (df['ê³ ê°€'] - df['ì €ê°€']) / df['ì‹œê°€']
    df['close_vs_open'] = (df['ì¢…ê°€'] - df['ì‹œê°€']) / df['ì‹œê°€']
    df['tail_up'] = (df['ê³ ê°€'] - df['ì¢…ê°€']) / df['ê³ ê°€']
    df['tail_down'] = (df['ì¢…ê°€'] - df['ì €ê°€']) / df['ì €ê°€']
    df['volume_price'] = df['ì¢…ê°€'] * df['ê±°ë˜ëŸ‰']
    return df

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('Supervised_Learning_CSV/Train_7days_K4.csv')  # ë¼ë²¨ í¬í•¨ëœ CSV

# 2. íŒŒìƒ ì§€í‘œ ì¶”ê°€
df = add_features(df)

# 3. ìƒìŠ¹/í•˜ë½ ë¼ë²¨ ìƒì„±
df['label'] = (df['X8'] > 0).astype(int)

# 4. X, y ë¶„ë¦¬
X = df.drop(columns=['ì¢…ëª©ì½”ë“œ', 'ëë‚ ì§œ', 'X8', 'label'])  # 'cluster'ëŠ” í¬í•¨ë¨
y = df['label']

# 5. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 6. ëª¨ë¸ ì •ì˜
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
lgbm_clf = LGBMClassifier(random_state=42)

# 7. ì•™ìƒë¸” (Soft Voting)
ensemble = VotingClassifier(
    estimators=[('xgb', xgb_clf), ('lgbm', lgbm_clf)],
    voting='soft'
)

# 8. ëª¨ë¸ í›ˆë ¨
ensemble.fit(X_train, y_train)

# 9. ì˜ˆì¸¡
y_pred = ensemble.predict(X_test)
y_prob = ensemble.predict_proba(X_test)[:, 1]

# ğŸ” í‰ê°€ ì§€í‘œ ì¶œë ¥
print("ğŸ“Š Accuracy: ", round(accuracy_score(y_test, y_pred), 4))
print("ğŸ¯ Precision:", round(precision_score(y_test, y_pred), 4))
print("ğŸ“ˆ Recall:   ", round(recall_score(y_test, y_pred), 4))
print("ğŸ“Œ F1 Score: ", round(f1_score(y_test, y_pred), 4))
print("ğŸš€ ROC AUC:  ", round(roc_auc_score(y_test, y_prob), 4))
