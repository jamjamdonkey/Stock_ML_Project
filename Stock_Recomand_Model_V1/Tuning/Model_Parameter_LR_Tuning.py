import pandas as pd
import numpy as np
import optuna
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
import joblib

# âœ… 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
df = pd.read_csv("Stock_Recomand_Model_V1/Learn.csv")
df.columns = df.columns.str.strip().str.lower()
df = df.apply(lambda col: pd.to_numeric(col, errors='coerce') if col.dtype == 'object' else col)
df = df.fillna(0)

# âœ… 2. í”¼ì²˜ / íƒ€ê²Ÿ ë¶„ë¦¬
drop_cols = ['ì¢…ëª©ì½”ë“œ', 'ë‚ ì§œ', 'target']
feature_cols = [col for col in df.columns if col not in drop_cols]
X = df[feature_cols]
y = df['target']

# âœ… 3. í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# âœ… 4. Optuna ëª©ì í•¨ìˆ˜ ì •ì˜
def objective(trial):
    penalty = trial.suggest_categorical("penalty", ["l1", "l2"])
    solver = "liblinear" if penalty == "l1" else "lbfgs"
    param = {
        'C': trial.suggest_float("C", 0.01, 10.0, log=True),
        'penalty': penalty,
        'solver': solver,
        'max_iter': 1000,
        'random_state': 42
    }
    model = LogisticRegression(**param)
    score = cross_val_score(model, X_scaled, y, cv=3, scoring='roc_auc')
    return score.mean()

# âœ… 5. íŠœë‹ ì‹¤í–‰
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# âœ… 6. ê²°ê³¼ ì¶œë ¥
print("ğŸ¯ Best ROC AUC:", study.best_value)
print("ğŸ”§ Best Params:")
for k, v in study.best_params.items():
    print(f" - {k}: {v}")

# âœ… 7. penaltyì— ë§ëŠ” solver ì¬ì§€ì •
penalty = study.best_params["penalty"]
solver = "liblinear" if penalty == "l1" else "lbfgs"

# âœ… 8. ìµœì  ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
best_model = LogisticRegression(
    penalty=penalty,
    C=study.best_params["C"],
    solver=solver,
    max_iter=1000,
    random_state=42
)
best_model.fit(X_scaled, y)

joblib.dump(best_model, "best_log_model.pkl")
print("âœ… ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: best_log_model.pkl")
