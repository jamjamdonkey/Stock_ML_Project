import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# ======================
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ======================
df = pd.read_csv("Supervised_Learning_CSV/Stock_Screener_FeatureSet_1D.csv", encoding='utf-8-sig')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
df = df.sort_values(by=['ì¢…ëª©ì½”ë“œ', 'ë‚ ì§œ'])

# ======================
# 2. ë¶„í¬ ì ê²€
# ======================
print("ğŸ¯ target ë¶„í¬ (ë¹„ìœ¨):")
print(df['target'].value_counts(normalize=True))

plt.figure(figsize=(12, 6))
sns.histplot(df['future_return_1d'], bins=60, kde=True, color="orange", edgecolor="black")
plt.title("í•˜ë£¨ ë’¤ ìˆ˜ìµë¥  ë¶„í¬")
plt.xlabel("ìˆ˜ìµë¥  (%)")
plt.ylabel("ë¹ˆë„")
plt.grid(True)
plt.show()

# ======================
# 3. ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘
# ======================
returns = []
accuracies = []

unique_dates = sorted(df['ë‚ ì§œ'].unique())

for i in tqdm(range(7, len(unique_dates)-3)):  # ë§ˆì§€ë§‰ 3ì¼ ì œì™¸ (ë¯¸ë˜ ìˆ˜ìµë¥ )
    train_dates = unique_dates[i-7:i]
    test_date = unique_dates[i]

    train = df[df['ë‚ ì§œ'].isin(train_dates)]
    test = df[df['ë‚ ì§œ'] == test_date]

    # ì¶”í›„ ë³µì›ì„ ìœ„í•´ ì¢…ëª©ì½”ë“œ, ìˆ˜ìµë¥  ë“± ì €ì¥
    test_codes = test['ì¢…ëª©ì½”ë“œ'].values
    test_returns = test['future_return_1d'].values
    test_targets = test['target'].values

    drop_cols = ['ë‚ ì§œ', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ê°€', 'target', 'future_return_1d']
    X_train = train.drop(columns=drop_cols).select_dtypes(include=[np.number])
    y_train = train['target']
    X_test = test.drop(columns=drop_cols).select_dtypes(include=[np.number])

    # ======================
    # 4. ì •ê·œí™”
    # ======================
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ======================
    # 5. ëª¨ë¸ í•™ìŠµ
    # ======================
    model_lgbm = LGBMClassifier(n_estimators=100, random_state=42)
    model_xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_lr = LogisticRegression(max_iter=1000, random_state=42)

    model_lgbm.fit(X_train_scaled, y_train)
    model_xgb.fit(X_train_scaled, y_train)
    model_lr.fit(X_train_scaled, y_train)

    # ======================
    # 6. ì†Œí”„íŠ¸ ë³´íŒ… ì˜ˆì¸¡
    # ======================
    proba_lgbm = model_lgbm.predict_proba(X_test_scaled)[:, 1]
    proba_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]
    proba_lr = model_lr.predict_proba(X_test_scaled)[:, 1]
    avg_proba = (proba_lgbm + proba_xgb + proba_lr) / 3

    # ======================
    # 7. ì¢…ëª©ì½”ë“œ ë³µì› ë° ì¶”ì²œ í‰ê°€
    # ======================
    test_result = pd.DataFrame({
        'ì¢…ëª©ì½”ë“œ': test_codes,
        'target': test_targets,
        'future_return_1d': test_returns,
        'avg_proba': avg_proba
    })

    # Top-N ì¶”ì²œ
    N = 1
    top_n = test_result.sort_values('avg_proba', ascending=False).head(N)

    avg_return = top_n['future_return_1d'].mean()
    acc = (top_n['target'] == 1).mean()

    returns.append(avg_return)
    accuracies.append(acc)

# ======================
# 8. ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥
# ======================
print(f"\nâœ… ì¢…ëª©ì½”ë“œ ë¶„ë¦¬ + LGBM + XGB + LR Soft Voting ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"ğŸ“Š í‰ê·  ìˆ˜ìµë¥ : {np.mean(returns):.4f}")
print(f"ğŸ¯ í‰ê·  ì •ë‹µë¥ : {np.mean(accuracies):.4f}")
