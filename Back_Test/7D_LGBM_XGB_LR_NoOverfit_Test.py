import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("Supervised_Learning_CSV/Stock_Screener_FeatureSet_1D.csv", encoding='utf-8-sig')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
df = df.sort_values(by=['ì¢…ëª©ì½”ë“œ', 'ë‚ ì§œ'])

# ì¢…ëª©ì½”ë“œ ê¸°ì¤€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
unique_codes = df['ì¢…ëª©ì½”ë“œ'].unique()
train_codes = unique_codes[:int(len(unique_codes)*0.8)]
test_codes = unique_codes[int(len(unique_codes)*0.8):]

returns = []
accuracies = []

unique_dates = sorted(df['ë‚ ì§œ'].unique())

for i in tqdm(range(7, len(unique_dates)-1)):  # 1ì¼ í›„ ìˆ˜ìµë¥  ê¸°ì¤€
    train_dates = unique_dates[i-7:i]
    test_date = unique_dates[i]

    train = df[(df['ë‚ ì§œ'].isin(train_dates)) & (df['ì¢…ëª©ì½”ë“œ'].isin(train_codes))]
    test = df[(df['ë‚ ì§œ'] == test_date) & (df['ì¢…ëª©ì½”ë“œ'].isin(test_codes))]

    if train.empty or test.empty:
        continue

    drop_cols = ['ë‚ ì§œ', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ê°€', 'target', 'future_return_1d']
    X_train = train.drop(columns=drop_cols).select_dtypes(include=[np.number])
    y_train = train['target']
    X_test = test.drop(columns=drop_cols).select_dtypes(include=[np.number])
    y_test = test['target']
    test_returns = test['future_return_1d'].values

    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ëª¨ë¸ ì •ì˜
    model_lgbm = LGBMClassifier(n_estimators=100, random_state=42)
    model_xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_lr = LogisticRegression(max_iter=1000, random_state=42)

    model_lgbm.fit(X_train_scaled, y_train)
    model_xgb.fit(X_train_scaled, y_train)
    model_lr.fit(X_train_scaled, y_train)

    # Soft Voting
    proba_lgbm = model_lgbm.predict_proba(X_test_scaled)[:, 1]
    proba_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]
    proba_lr = model_lr.predict_proba(X_test_scaled)[:, 1]
    avg_proba = (proba_lgbm + proba_xgb + proba_lr) / 3

    test = test.copy()
    test['avg_proba'] = avg_proba
    test['future_return_1d'] = test_returns

    N = 1  # Top N ì¶”ì²œ ì¢…ëª© ìˆ˜
    top_n = test.sort_values('avg_proba', ascending=False).head(N)

    avg_return = top_n['future_return_1d'].mean()
    acc = (top_n['target'] == 1).mean()

    returns.append(avg_return)
    accuracies.append(acc)

# ê²°ê³¼ ì¶œë ¥
print(f"\nâœ… ì¢…ëª©ì½”ë“œ ë¶„ë¦¬ + LGBM + XGB + LR Soft Voting ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"ğŸ“Š í‰ê·  ìˆ˜ìµë¥ : {np.mean(returns):.4f}")
print(f"ğŸ¯ í‰ê·  ì •ë‹µë¥ : {np.mean(accuracies):.4f}")

