import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("Supervised_Learning_CSV/Stock_Screener_FeatureSet_1D.csv", encoding='utf-8-sig')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
df = df.sort_values(by=['ì¢…ëª©ì½”ë“œ', 'ë‚ ì§œ'])

# 2. ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
returns = []
accuracies = []

# 3. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸
unique_dates = sorted(df['ë‚ ì§œ'].unique())
for i in tqdm(range(7, len(unique_dates)-3)):  # -3ì€ future_return_3d ëŒ€ìƒ í™•ë³´
    train_dates = unique_dates[i-7:i]
    test_date = unique_dates[i]

    train = df[df['ë‚ ì§œ'].isin(train_dates)]
    test = df[df['ë‚ ì§œ'] == test_date]

    drop_cols = ['ë‚ ì§œ', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ê°€', 'target', 'future_return_1d']
    X_train = train.drop(columns=drop_cols).select_dtypes(include=[np.number])
    y_train = train['target']
    X_test = test.drop(columns=drop_cols).select_dtypes(include=[np.number])
    y_test = test['target']
    test_returns = test['future_return_1d'].values
    test_codes = test['ì¢…ëª©ì½”ë“œ'].values

    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ëª¨ë¸ ì •ì˜
    model_lgbm = LGBMClassifier(n_estimators=100, random_state=42)
    model_xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_lr = LogisticRegression(max_iter=1000, random_state=42)

    model_lgbm.fit(X_train_scaled, y_train)
    model_xgb.fit(X_train_scaled, y_train)
    model_lr.fit(X_train_scaled, y_train)

    # ì†Œí”„íŠ¸ë³´íŒ…
    proba_lgbm = model_lgbm.predict_proba(X_test_scaled)[:, 1]
    proba_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]
    proba_lr = model_lr.predict_proba(X_test_scaled)[:, 1]
    avg_proba = (proba_lgbm + proba_xgb + proba_lr) / 3

    test = test.copy()
    test['avg_proba'] = avg_proba
    test['future_return_1d'] = test_returns
    test['ì¢…ëª©ì½”ë“œ'] = test_codes

    # Top N ì¶”ì²œ
    N = 1  # ì¶”ì²œ ì¢…ëª© ìˆ˜ ì„¤ì •
    top_n = test.sort_values('avg_proba', ascending=False).head(N)

    # í‰ê°€
    avg_return = top_n['future_return_1d'].mean()
    acc = (top_n['target'] == 1).mean()

    returns.append(avg_return)
    accuracies.append(acc)

# 4. ê²°ê³¼ ì¶œë ¥
print(f"\nâœ… LGBM + XGB + LR Soft Voting ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"ğŸ“Š í‰ê·  ìˆ˜ìµë¥ : {np.mean(returns):.4f}")
print(f"ğŸ¯ í‰ê·  ì •ë‹µë¥ : {np.mean(accuracies):.4f}")
