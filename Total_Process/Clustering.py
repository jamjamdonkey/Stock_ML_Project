import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import os

# âœ… ìƒëŒ€ê²½ë¡œ ê¸°ë°˜ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('Total_Process/3Y_Merged_Stock_Data.csv', encoding="utf-8-sig")

# âœ… ë‚ ì§œ ì •ë ¬
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
df = df.sort_values(['ì¢…ëª©ì½”ë“œ', 'ë‚ ì§œ'])

# âœ… 3ì¼ ìˆ˜ìµë¥  ê³„ì‚°
df['ìˆ˜ìµë¥ _3d'] = df.groupby('ì¢…ëª©ì½”ë“œ')['ì¢…ê°€'].pct_change(periods=3)

# âœ… ì¢…ëª©ì½”ë“œë³„ë¡œ íŠ¹ì§• ì¶”ì¶œ
feature_df = df.groupby('ì¢…ëª©ì½”ë“œ').agg({
    'ìˆ˜ìµë¥ _3d': ['mean', 'std'],
    'ê±°ë˜ëŸ‰': 'mean',
    'ì¢…ê°€': ['mean', 'std'],
    'ì‹œê°€': 'mean',
    'ê³ ê°€': 'mean',
    'ì €ê°€': 'mean',
}).dropna()

# âœ… ë‹¤ì¤‘ ì¸ë±ìŠ¤ ì •ë¦¬
feature_df.columns = ['_'.join(col) for col in feature_df.columns]
feature_df = feature_df.reset_index()

# âœ… ì •ê·œí™”
X = feature_df.drop(columns='ì¢…ëª©ì½”ë“œ')
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# âœ… í´ëŸ¬ìŠ¤í„°ë§
kmeans = KMeans(n_clusters=4, random_state=42)
feature_df['í´ëŸ¬ìŠ¤í„°'] = kmeans.fit_predict(X_scaled)

# âœ… ê²°ê³¼ ì €ì¥ (ìƒëŒ€ ê²½ë¡œ)
save_dir = "Total_Process"
os.makedirs(save_dir, exist_ok=True)
output_path = os.path.join(save_dir, "Stock_Clusters.csv")
feature_df.to_csv(output_path, index=False, encoding='utf-8-sig')

# âœ… ì½˜ì†” ì¶œë ¥
print("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½ (ìƒìœ„ 5ê°œ):")
print(feature_df.head())

print("\nğŸ“ˆ í´ëŸ¬ìŠ¤í„°ë³„ ì¢…ëª© ìˆ˜:")
print(feature_df['í´ëŸ¬ìŠ¤í„°'].value_counts().sort_index())

print("\nâœ… ì €ì¥ëœ CSV ê²½ë¡œ:")
print(output_path)
